model:
  model_class: DeepBert
  model_setting:
    dim: 768
    n_heads: 12
    dim_ff: 3072
    pred_drop: 0.1
    vocab_size: -1
    t_activation: 'gelu'
    loss_type: CrossEntropyLoss
    num_basic_block_layer: 12
    pad_idx: from:data.special_token_idx.PAD
train:
  is_bert: true
  seed: 3431
  batch_size: 8
  val_batch_size: 8
  n_epochs: 30
  clip_grad_norm: 0.2
  optimizer: Adam
  optimizer_setting:
    lr: 0.00001
    weight_decay: 0.01
  lr_scheduler: get_warmp_with_decay_lr_sched
  use_batch_step_lr: true
  lr_scheduler_setting:
    warmup_ratio: 0.1
  hyperparameter_testing:
    using: False
    mult: 0.2
data:
  data_file: training_data/spec_core_0714.data
  # src_info_file: training_data/skl_mapping.csv
  data_setting:
    bert: false
    only_unique: true
    split_mode: short_in_train:199
    train_perc: 8
    val_perc: 2
    test_perc: 0
    prepare_mode: stacked_extra_tags
    shuffle: false
    instr_limit: 400
  dataset_class: StackedBertDatasetTest
  dataset_setting:
    too_long_limit: 512
    vocab_size: -1
    mask_rate: 0.15
  # raw_data: false
  #  sample usage of predifined special token idxs
  special_token_idx:
    PAD: 0
    SRCS: 1
    DSTS: 2
    MEM: 3 # <MEM>
    /MEM: 4  # </MEM>
    START: 5
    END: 6
    UNK: 7 
    OP: 8
    BASE: 9
    INDEX: 10
    DISP: 11
    SCALE: 12

    