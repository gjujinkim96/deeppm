model:
  model_class: DeepPM
  model_setting:
    dim: 512
    n_heads: 8
    dim_ff: 2048
    vocab_size: 700
    pad_idx: from:data.special_token_idx.PAD
    use_checkpoint: true
    num_basic_block_layer: 2
    num_instruction_layer: 2
    num_op_layer: 4
    dropout: 0.0
    pred_drop: 0.0
    use_layernorm: false
    use_pos_2d: false
    activation: gelu
    use_bb_attn: true
    use_seq_attn: true
    use_op_attn: true
    handle_neg: true
train:
  seed: 3431
  batch_size: 16
  val_batch_size: 16
  n_epochs: 30
  clip_grad_norm: 0.2
  optimizer: Adam
  optimizer_setting:
    lr: 0.00001
  lr_scheduler: LinearLR
  use_batch_step_lr: false
  lr_scheduler_setting:
    total_iters: 5
  gradient_accumlation:
    using: False
    steps: 4
  loss: MapeLoss
  # loss_setting:
  save_epoch: -1
    # x == 0: no save
    # x == -1: just save best and last epoch
    # x > 0: save every `x`th epoch and also best and last
  cpu_count: null
    # x == null: use multiprocessing.cpu_count
    # x == number: use that value
  kfold:
    using: false
    k: 5
data:
  data_file: training_data/intel_core.data
  data_setting:
    only_unique: true
    split_mode: num_instrs
    train_perc: 7
    val_perc: 1.5
    test_perc: 1.5
    prepare_mode: stacked
    shuffle: true
    instr_limit: 400
    custom_idx_split: model_resources/data/7_1.5_1.5_idx_dict.dump
    # custom_idx_split: model_resources/data/idx_dict.dump
    given_token_mapping: null
  dataset_class: DatasetWithDistanceWeight
  dataset_setting:
    too_long_limit: 512
    return_bb_mask: true
    return_seq_mask: true
    return_op_mask: true
  special_token_idx:
    PAD: 0
    BLOCK_START: 1
    BLOCK_END: 2
    START: 3
    END: 4  
    SEP: 5
    UNK: 6 
    MSK: 7
